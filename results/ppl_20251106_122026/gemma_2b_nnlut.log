`torch_dtype` is deprecated! Use `dtype` instead!
============================================================
[INFO] Loading Model
============================================================
[INFO] Model Name: Gemma-2B
[INFO] Model Path: google/gemma-2b
[INFO] Attention Implementation: eager
[INFO] Data Type: bfloat16 (torch.bfloat16)
[INFO] Using NN-LUT for activation functions
[INFO]   - SiLU LUT: nnlut_bench/lut_details_silu_H32_sub.json
[INFO]   - Exp LUT: nnlut_bench/lut_details_exp_H32_sub.json
[INFO] Sequence Length: 2048
============================================================
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 106.63it/s]

============================================================
[INFO] Model Configuration
============================================================
[INFO] Model Type: gemma
[INFO] Architecture: ['GemmaForCausalLM']
[INFO] Hidden Size: 2048
[INFO] Number of Layers: 18
[INFO] Number of Attention Heads: 8
[INFO] Vocabulary Size: 256000
[INFO] Max Position Embeddings: 8192
[INFO] Model Loaded Dtype: torch.bfloat16
[INFO] Total Parameters: 2,506,172,416 (2.51B)
[INFO] Trainable Parameters: 2,506,172,416
============================================================

[WARNING] Detected unknown architecture, using llama_attn_forward
[DEBUG] Config hidden_act: gelu
[INFO] Detected activation function: GELU
Traceback (most recent call last):
  File "/home/lxf/workspace/NN-LUT/eval_ppl.py", line 557, in <module>
    main(args)
  File "/home/lxf/workspace/NN-LUT/eval_ppl.py", line 505, in main
    raise ValueError(
ValueError: Model uses GELU activation function but --lut_gelu_path not provided. Please provide the correct GELU LUT path using --lut_gelu_path.
[W1106 13:01:49.149131989 AllocatorConfig.cpp:28] Warning: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF instead (function operator())
